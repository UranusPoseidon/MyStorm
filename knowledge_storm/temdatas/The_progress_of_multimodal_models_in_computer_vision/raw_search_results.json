{"uid_428": {"url": "uid_428", "description": NaN, "snippets": ["quantify volumes and shapes used in diagnosis and monitoring treatment;\nregistration of multimodality images of organs improves detection, diagnosis\nand staging of diseases as well as image-guided surgery and therapy,\nregistration of images obtained from the same modality are used to monitor\nprogression of therapy. These challenging clinical-motivated applications\nintroduce novel and sophisticated mathematical problems which stimulate"], "title": "", "meta": {"query": "examples of multimodal models in healthcare applications"}, "citation_uuid": -1}, "uid_7698": {"url": "uid_7698", "description": NaN, "snippets": ["quantify volumes and shapes used in diagnosis and monitoring treatment;\nregistration of multimodality images of organs improves detection, diagnosis\nand staging of diseases as well as image-guided surgery and therapy,\nregistration of images obtained from the same modality are used to monitor\nprogression of therapy. These challenging clinical-motivated applications\nintroduce novel and sophisticated mathematical problems which stimulate"], "title": "", "meta": {"query": "examples of multimodal models in healthcare applications"}, "citation_uuid": -1}, "uid_1016": {"url": "uid_1016", "description": NaN, "snippets": ["Recently a number of studies demonstrated impressive performance on diverse\nvision-language multi-modal tasks such as image captioning and visual question\nanswering by extending the BERT architecture with multi-modal pre-training\nobjectives. In this work we explore a broad set of multi-modal representation\nlearning tasks in the medical domain, specifically using radiology images and\nthe unstructured report. We propose Medical Vision Language Learner (MedViLL)"], "title": "", "meta": {"query": "multimodal models in computer vision healthcare case studies"}, "citation_uuid": -1}, "uid_5333": {"url": "uid_5333", "description": NaN, "snippets": ["Recently a number of studies demonstrated impressive performance on diverse\nvision-language multi-modal tasks such as image captioning and visual question\nanswering by extending the BERT architecture with multi-modal pre-training\nobjectives. In this work we explore a broad set of multi-modal representation\nlearning tasks in the medical domain, specifically using radiology images and\nthe unstructured report. We propose Medical Vision Language Learner (MedViLL)"], "title": "", "meta": {"query": "multimodal models in computer vision healthcare case studies"}, "citation_uuid": -1}, "uid_15934": {"url": "uid_15934", "description": NaN, "snippets": ["environment and apply it to the real-world. Moreover, our approach, with merely\r\na 3D model being required, has the potential to generalize to other types of\r\nmulti-rigid-body dynamic systems.", "environment and apply it to the real-world. Moreover, our approach, with merely\na 3D model being required, has the potential to generalize to other types of\nmulti-rigid-body dynamic systems."], "title": "", "meta": {"query": "realworld applications of multimodal models in robotics"}, "citation_uuid": -1}, "uid_12957": {"url": "uid_12957", "description": NaN, "snippets": ["environment and apply it to the real-world. Moreover, our approach, with merely\r\na 3D model being required, has the potential to generalize to other types of\r\nmulti-rigid-body dynamic systems.", "environment and apply it to the real-world. Moreover, our approach, with merely\na 3D model being required, has the potential to generalize to other types of\nmulti-rigid-body dynamic systems."], "title": "", "meta": {"query": "realworld applications of multimodal models in robotics"}, "citation_uuid": -1}, "uid_1883": {"url": "uid_1883", "description": NaN, "snippets": ["The inability to interpret the model prediction in semantically and visually\nmeaningful ways is a well-known shortcoming of most existing computer-aided\ndiagnosis methods. In this paper, we propose MDNet to establish a direct\nmultimodal mapping between medical images and diagnostic reports that can read\nimages, generate diagnostic reports, retrieve images by symptom descriptions,\nand visualize attention, to provide justifications of the network diagnosis", "The inability to interpret the model prediction in semantically and visually\r\nmeaningful ways is a well-known shortcoming of most existing computer-aided\r\ndiagnosis methods. In this paper, we propose MDNet to establish a direct\r\nmultimodal mapping between medical images and diagnostic reports that can read\r\nimages, generate diagnostic reports, retrieve images by symptom descriptions,\r\nand visualize attention, to provide justifications of the network diagnosis"], "title": "", "meta": {"query": "multimodal models in healthcare diagnostics technologies"}, "citation_uuid": -1}, "uid_4416": {"url": "uid_4416", "description": NaN, "snippets": ["which we pretrain a 3-D model in the source HSI data sets containing a greater\nnumber of labeled samples and then transfer it to the target HSI data sets and\n2) cross-modal strategy, in which we pretrain a 3-D model in the 2-D RGB image\ndata sets containing a large number of samples and then transfer it to the\ntarget HSI data sets. In contrast to previous approaches, we do not impose\nrestrictions over the source data sets, in which they do not have to be", "which we pretrain a 3-D model in the source HSI data sets containing a greater\r\nnumber of labeled samples and then transfer it to the target HSI data sets and\r\n2) cross-modal strategy, in which we pretrain a 3-D model in the 2-D RGB image\r\ndata sets containing a large number of samples and then transfer it to the\r\ntarget HSI data sets. In contrast to previous approaches, we do not impose\r\nrestrictions over the source data sets, in which they do not have to be"], "title": "", "meta": {"query": "specific methods in multimodal models for healthcare"}, "citation_uuid": -1}, "uid_4835": {"url": "uid_4835", "description": NaN, "snippets": ["The recent success of Transformers in the language domain has motivated\nadapting it to a multimodal setting, where a new visual model is trained in\ntandem with an already pretrained language model. However, due to the excessive\nmemory requirements from Transformers, existing work typically fixes the\nlanguage model and train only the vision module, which limits its ability to\nlearn cross-modal information in an end-to-end manner. In this work, we focus"], "title": "", "meta": {"query": "limitations of multimodal models in robotics"}, "citation_uuid": -1}, "uid_4980": {"url": "uid_4980", "description": NaN, "snippets": ["Transformer networks have proven extremely powerful for a wide variety of\ntasks since they were introduced. Computer vision is not an exception, as the\nuse of transformers has become very popular in the vision community in recent\nyears. Despite this wave, multiple-object tracking (MOT) exhibits for now some\nsort of incompatibility with transformers. We argue that the standard\nrepresentation - bounding boxes with insufficient sparse queries - is not", "Transformer networks have proven extremely powerful for a wide variety of\r\ntasks since they were introduced. Computer vision is not an exception, as the\r\nuse of transformers has become very popular in the vision community in recent\r\nyears. Despite this wave, multiple-object tracking (MOT) exhibits for now some\r\nsort of incompatibility with transformers. We argue that the standard\r\nrepresentation - bounding boxes with insufficient sparse queries - is not"], "title": "", "meta": {"query": "multimodal transformers success in computer vision"}, "citation_uuid": -1}, "uid_1641": {"url": "uid_1641", "description": NaN, "snippets": ["The focus of this survey is on the analysis of two modalities of multimodal\r\ndeep learning: image and text. Unlike classic reviews of deep learning where\r\nmonomodal image classifiers such as VGG, ResNet and Inception module are\r\ncentral topics, this paper will examine recent multimodal deep models and\r\nstructures, including auto-encoders, generative adversarial nets and their\r\nvariants. These models go beyond the simple image classifiers in which they can", "The focus of this survey is on the analysis of two modalities of multimodal\ndeep learning: image and text. Unlike classic reviews of deep learning where\nmonomodal image classifiers such as VGG, ResNet and Inception module are\ncentral topics, this paper will examine recent multimodal deep models and\nstructures, including auto-encoders, generative adversarial nets and their\nvariants. These models go beyond the simple image classifiers in which they can"], "title": "", "meta": {"query": "theoretical foundations of multimodal models in vision"}, "citation_uuid": -1}, "uid_1574": {"url": "uid_1574", "description": NaN, "snippets": ["computer vision. In particular, we introduce a Multi-modal Multi-scale\r\nTRansformer (M2TR), which uses a multi-scale transformer that operates on\r\npatches of different sizes to detect the local inconsistency at different\r\nspatial levels. To improve the detection results and enhance the robustness of\r\nour method to image compression, M2TR also takes frequency information, which\r\nis further combined with RGB features using a cross modality fusion module.", "computer vision. In particular, we introduce a Multi-modal Multi-scale\nTRansformer (M2TR), which uses a multi-scale transformer that operates on\npatches of different sizes to detect the local inconsistency at different\nspatial levels. To improve the detection results and enhance the robustness of\nour method to image compression, M2TR also takes frequency information, which\nis further combined with RGB features using a cross modality fusion module."], "title": "", "meta": {"query": "key theoretical advancements in multimodal transformers computer vision"}, "citation_uuid": -1}, "uid_5424": {"url": "uid_5424", "description": NaN, "snippets": ["computer vision. In particular, we introduce a Multi-modal Multi-scale\r\nTRansformer (M2TR), which uses a multi-scale transformer that operates on\r\npatches of different sizes to detect the local inconsistency at different\r\nspatial levels. To improve the detection results and enhance the robustness of\r\nour method to image compression, M2TR also takes frequency information, which\r\nis further combined with RGB features using a cross modality fusion module.", "computer vision. In particular, we introduce a Multi-modal Multi-scale\nTRansformer (M2TR), which uses a multi-scale transformer that operates on\npatches of different sizes to detect the local inconsistency at different\nspatial levels. To improve the detection results and enhance the robustness of\nour method to image compression, M2TR also takes frequency information, which\nis further combined with RGB features using a cross modality fusion module."], "title": "", "meta": {"query": "key theoretical advancements in multimodal transformers computer vision"}, "citation_uuid": -1}, "uid_15937": {"url": "uid_15937", "description": NaN, "snippets": ["combining specifications and visual information of items have succeeded in\r\nobtaining fine-grained representations of fashion items, although they\r\ngenerally apply simple vector operations through a multimodal fusion. We\r\nsimilarly build a multimodal model using images and attributes of the product\r\nand further employ state-of-the-art multimodal deep neural networks applied in\r\ncomputer vision to achieve a practical performance level. In addition, we model", "combining specifications and visual information of items have succeeded in\nobtaining fine-grained representations of fashion items, although they\ngenerally apply simple vector operations through a multimodal fusion. We\nsimilarly build a multimodal model using images and attributes of the product\nand further employ state-of-the-art multimodal deep neural networks applied in\ncomputer vision to achieve a practical performance level. In addition, we model"], "title": "", "meta": {"query": "multimodal models in computer vision applications examples"}, "citation_uuid": -1}, "uid_4834": {"url": "uid_4834", "description": NaN, "snippets": ["largely improved performance in simultaneous 2D object localization and\r\nviewpoint estimation on a recent dataset of challenging street scenes.", "largely improved performance in simultaneous 2D object localization and\nviewpoint estimation on a recent dataset of challenging street scenes."], "title": "", "meta": {"query": "recent advancements in multimodal models for computer vision tasks"}, "citation_uuid": -1}, "uid_5516": {"url": "uid_5516", "description": NaN, "snippets": ["The observation that computer vision methods overfit to dataset specifics has\r\ninspired diverse attempts to make object recognition models robust to domain\r\nshifts. However, similar work on domain-robust visual question answering\r\nmethods is very limited. Domain adaptation for VQA differs from adaptation for\r\nobject recognition due to additional complexity: VQA models handle multimodal\r\ninputs, methods contain multiple steps with diverse modules resulting in", "The observation that computer vision methods overfit to dataset specifics has\ninspired diverse attempts to make object recognition models robust to domain\nshifts. However, similar work on domain-robust visual question answering\nmethods is very limited. Domain adaptation for VQA differs from adaptation for\nobject recognition due to additional complexity: VQA models handle multimodal\ninputs, methods contain multiple steps with diverse modules resulting in"], "title": "", "meta": {"query": "limitations of multimodal models in computer vision"}, "citation_uuid": -1}, "uid_2006": {"url": "uid_2006", "description": NaN, "snippets": ["methods on the SUN RGB-D dataset. On ScanNet, ImVoxelNet sets a new benchmark\nfor multi-view 3D object detection. The source code and the trained models are\navailable at https://github.com/saic-vul/imvoxelnet.", "methods on the SUN RGB-D dataset. On ScanNet, ImVoxelNet sets a new benchmark\r\nfor multi-view 3D object detection. The source code and the trained models are\r\navailable at https://github.com/saic-vul/imvoxelnet."], "title": "", "meta": {"query": "latest advancements in multimodal models for computer vision"}, "citation_uuid": -1}, "uid_14306": {"url": "uid_14306", "description": NaN, "snippets": ["Existing learning models often utilise CT-scan images to predict lung\ndiseases. These models are posed by high uncertainties that affect lung\nsegmentation and visual feature learning. We introduce MARL, a novel Multimodal\nAttentional Representation Learning model architecture that learns useful\nfeatures from multimodal data under uncertainty. We feed the proposed model\nwith both the lung CT-scan images and their perspective historical patients'"], "title": "", "meta": {"query": "significant multimodal models in computer vision 2023"}, "citation_uuid": -1}, "uid_1037": {"url": "uid_1037", "description": NaN, "snippets": ["Existing learning models often utilise CT-scan images to predict lung\r\ndiseases. These models are posed by high uncertainties that affect lung\r\nsegmentation and visual feature learning. We introduce MARL, a novel Multimodal\r\nAttentional Representation Learning model architecture that learns useful\r\nfeatures from multimodal data under uncertainty. We feed the proposed model\r\nwith both the lung CT-scan images and their perspective historical patients'"], "title": "", "meta": {"query": "significant multimodal models in computer vision 2023"}, "citation_uuid": -1}, "uid_1043": {"url": "uid_1043", "description": NaN, "snippets": ["correlation simultaneously, hence improving the quality of learned visual\r\nrepresentations. By including multimodal training in a unified framework with\r\ndifferent types of contrastive losses, our method can learn more powerful and\r\ngeneric visual features. We first train our model on COCO and evaluate the\r\nlearned visual representations on various downstream tasks including image\r\nclassification, object detection, and instance segmentation. For example, the", "correlation simultaneously, hence improving the quality of learned visual\nrepresentations. By including multimodal training in a unified framework with\ndifferent types of contrastive losses, our method can learn more powerful and\ngeneric visual features. We first train our model on COCO and evaluate the\nlearned visual representations on various downstream tasks including image\nclassification, object detection, and instance segmentation. For example, the"], "title": "", "meta": {"query": "comparison of multimodal models in computer vision"}, "citation_uuid": -1}, "uid_2316": {"url": "uid_2316", "description": NaN, "snippets": ["correlation simultaneously, hence improving the quality of learned visual\nrepresentations. By including multimodal training in a unified framework with\ndifferent types of contrastive losses, our method can learn more powerful and\ngeneric visual features. We first train our model on COCO and evaluate the\nlearned visual representations on various downstream tasks including image\nclassification, object detection, and instance segmentation. For example, the"], "title": "", "meta": {"query": "comparison of multimodal models in computer vision"}, "citation_uuid": -1}, "uid_1351": {"url": "uid_1351", "description": NaN, "snippets": ["Multimodal representation learning is gaining more and more interest within\r\nthe deep learning community. While bilinear models provide an interesting\r\nframework to find subtle combination of modalities, their number of parameters\r\ngrows quadratically with the input dimensions, making their practical\r\nimplementation within classical deep learning pipelines challenging. In this\r\npaper, we introduce BLOCK, a new multimodal fusion based on the", "Multimodal representation learning is gaining more and more interest within\nthe deep learning community. While bilinear models provide an interesting\nframework to find subtle combination of modalities, their number of parameters\ngrows quadratically with the input dimensions, making their practical\nimplementation within classical deep learning pipelines challenging. In this\npaper, we introduce BLOCK, a new multimodal fusion based on the"], "title": "", "meta": {"query": "impact of multimodal models on industries"}, "citation_uuid": -1}, "uid_8668": {"url": "uid_8668", "description": NaN, "snippets": ["Multi-Camera Multiple Object Tracking (MC-MOT) is a significant computer\nvision problem due to its emerging applicability in several real-world\napplications. Despite a large number of existing works, solving the data\nassociation problem in any MC-MOT pipeline is arguably one of the most\nchallenging tasks. Developing a robust MC-MOT system, however, is still highly\nchallenging due to many practical issues such as inconsistent lighting"], "title": "", "meta": {"query": "challenges of implementing multimodal models in computer vision"}, "citation_uuid": -1}, "uid_14460": {"url": "uid_14460", "description": NaN, "snippets": ["methods only focus on improving accuracy with minimal parameter update, while\nignoring high computing and memory cost during training, which makes it\nimpossible to deploy multi-domain learning into more and more widely used\nresource-limited edge devices, like mobile phone, IoT, embedded system, etc.\nDuring our study in multi-domain training, we observe that large memory used\nfor activation storage is the bottleneck that largely limits the training time"], "title": "", "meta": {"query": "limitations of multimodal models in industry applications"}, "citation_uuid": -1}, "uid_17024": {"url": "uid_17024", "description": NaN, "snippets": ["Robust multiple model fitting plays a crucial role in many computer vision\napplications. Unlike single model fitting problems, the multi-model fitting has\nadditional challenges. The unknown number of models and the inlier noise scale\nare the two most important of them, which are in general provided by the user\nusing ground-truth or some other auxiliary information. Mode seeking/\nclustering-based approaches crucially depend on the quality of model hypotheses"], "title": "", "meta": {"query": "multimodal models challenges and solutions in computer vision"}, "citation_uuid": -1}, "uid_11146": {"url": "uid_11146", "description": NaN, "snippets": ["current handcrafted deep models on challenging benchmarks across multiple tasks\r\nwith remarkable margins."], "title": "", "meta": {"query": "current issues in multimodal AI models ethics"}, "citation_uuid": -1}, "uid_16902": {"url": "uid_16902", "description": NaN, "snippets": ["Algorithmic decision systems have frequently been labelled as \"biased\",\n\"racist\", \"sexist\", or \"unfair\" by numerous media outlets, organisations, and\nresearchers. There is an ongoing debate about whether such assessments are\njustified and whether citizens and policymakers should be concerned. These and\nother related matters have recently become a hot topic in the context of\nbiometric technologies, which are ubiquitous in personal, commercial, and"], "title": "", "meta": {"query": "current issues in multimodal AI models ethics"}, "citation_uuid": -1}, "uid_16002": {"url": "uid_16002", "description": NaN, "snippets": ["machine translation, here we extend it to a Multimodal Transformer (MT) model\r\nfor image captioning. Compared to existing image captioning approaches, the MT\r\nmodel simultaneously captures intra- and inter-modal interactions in a unified\r\nattention block. Due to the in-depth modular composition of such attention\r\nblocks, the MT model can perform complex multimodal reasoning and output\r\naccurate captions. Moreover, to further improve the image captioning"], "title": "", "meta": {"query": "multimodal models ethical implications 2023"}, "citation_uuid": -1}, "uid_8356": {"url": "uid_8356", "description": NaN, "snippets": ["machine translation, here we extend it to a Multimodal Transformer (MT) model\r\nfor image captioning. Compared to existing image captioning approaches, the MT\r\nmodel simultaneously captures intra- and inter-modal interactions in a unified\r\nattention block. Due to the in-depth modular composition of such attention\r\nblocks, the MT model can perform complex multimodal reasoning and output\r\naccurate captions. Moreover, to further improve the image captioning"], "title": "", "meta": {"query": "multimodal models ethical implications 2023"}, "citation_uuid": -1}, "uid_14910": {"url": "uid_14910", "description": NaN, "snippets": ["to traditional second-order approaches on common computer vision problems, such\nas image alignment and essential matrix estimation, with very large numbers of\nresiduals."], "title": "", "meta": {"query": "examples of algorithmic bias in multimodal models computer vision"}, "citation_uuid": -1}, "uid_9975": {"url": "uid_9975", "description": NaN, "snippets": ["into the behavior of such models. Our results also show examples of unusual\r\nbehaviors learned by models in attempting VQA tasks."], "title": "", "meta": {"query": "case studies algorithmic bias multimodal models"}, "citation_uuid": -1}, "uid_16322": {"url": "uid_16322", "description": NaN, "snippets": ["into the behavior of such models. Our results also show examples of unusual\r\nbehaviors learned by models in attempting VQA tasks."], "title": "", "meta": {"query": "case studies algorithmic bias multimodal models"}, "citation_uuid": -1}, "uid_13885": {"url": "uid_13885", "description": NaN, "snippets": ["the impact of different datasets, models, attack methods and defense methods.\nDirectly, our work proves the limited robustness of unsupervised domain\nadaptation model, and we hope our work may facilitate the community to pay more\nattention to improve the robustness of the model against attacking."], "title": "", "meta": {"query": "societal impact of multimodal AI models in law enforcement"}, "citation_uuid": -1}, "uid_2877": {"url": "uid_2877", "description": NaN, "snippets": ["distorted. These rare \"edge-case\" scenarios are not represented in available\r\ndatasets, and existing fusion architectures are not designed to handle them. To\r\naddress this challenge we present a novel multimodal dataset acquired in over\r\n10,000km of driving in northern Europe. Although this dataset is the first\r\nlarge multimodal dataset in adverse weather, with 100k labels for lidar,\r\ncamera, radar, and gated NIR sensors, it does not facilitate training as", "distorted. These rare \"edge-case\" scenarios are not represented in available\ndatasets, and existing fusion architectures are not designed to handle them. To\naddress this challenge we present a novel multimodal dataset acquired in over\n10,000km of driving in northern Europe. Although this dataset is the first\nlarge multimodal dataset in adverse weather, with 100k labels for lidar,\ncamera, radar, and gated NIR sensors, it does not facilitate training as"], "title": "", "meta": {"query": "notable incidents involving multimodal models in surveillance and law enforcement"}, "citation_uuid": -1}, "uid_16296": {"url": "uid_16296", "description": NaN, "snippets": ["This survey allows us to identify challenges and discuss future research\ndirections for the development of robust facial models in real-world\nconditions."], "title": "", "meta": {"query": "ethical challenges of multimodal models in realworld applications"}, "citation_uuid": -1}}