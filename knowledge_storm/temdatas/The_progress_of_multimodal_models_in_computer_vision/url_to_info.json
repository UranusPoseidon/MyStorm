{"url_to_unified_index": {"uid_16349": 2, "uid_15937": 3, "uid_1641": 1, "uid_5333": 4, "uid_890": 11, "uid_4980": 5, "uid_1574": 6, "uid_11109": 12, "uid_17244": 9, "uid_2006": 13, "uid_1043": 10, "uid_8356": 7, "uid_1016": 8, "uid_3426": 14, "uid_7698": 15, "uid_15934": 18, "uid_12957": 19, "uid_428": 16, "uid_1883": 17, "uid_11511": 20, "uid_11628": 21, "uid_17024": 22, "uid_16467": 23, "uid_9266": 24, "uid_16902": 25, "uid_14040": 26, "uid_5183": 27}, "url_to_info": {"uid_16349": {"url": "uid_16349", "description": NaN, "snippets": ["Vision-based motion estimation and 3D reconstruction, which have numerous\napplications (e.g., autonomous driving, navigation systems for airborne devices\nand augmented reality) are receiving significant research attention. To\nincrease the accuracy and robustness, several researchers have recently\ndemonstrated the benefit of using large field-of-view cameras for such\napplications. In this paper, we provide an extensive review of existing models"], "title": "", "meta": {"query": "notable realworld applications of multimodal models in computer vision"}, "citation_uuid": -1}, "uid_15937": {"url": "uid_15937", "description": NaN, "snippets": ["combining specifications and visual information of items have succeeded in\nobtaining fine-grained representations of fashion items, although they\ngenerally apply simple vector operations through a multimodal fusion. We\nsimilarly build a multimodal model using images and attributes of the product\nand further employ state-of-the-art multimodal deep neural networks applied in\ncomputer vision to achieve a practical performance level. In addition, we model"], "title": "", "meta": {"query": "multimodal models in computer vision case studies"}, "citation_uuid": -1}, "uid_1641": {"url": "uid_1641", "description": NaN, "snippets": ["The focus of this survey is on the analysis of two modalities of multimodal\ndeep learning: image and text. Unlike classic reviews of deep learning where\nmonomodal image classifiers such as VGG, ResNet and Inception module are\ncentral topics, this paper will examine recent multimodal deep models and\nstructures, including auto-encoders, generative adversarial nets and their\nvariants. These models go beyond the simple image classifiers in which they can"], "title": "", "meta": {"query": "theoretical foundations of multimodal models in vision"}, "citation_uuid": -1}, "uid_5333": {"url": "uid_5333", "description": NaN, "snippets": ["Recently a number of studies demonstrated impressive performance on diverse\nvision-language multi-modal tasks such as image captioning and visual question\nanswering by extending the BERT architecture with multi-modal pre-training\nobjectives. In this work we explore a broad set of multi-modal representation\nlearning tasks in the medical domain, specifically using radiology images and\nthe unstructured report. We propose Medical Vision Language Learner (MedViLL)"], "title": "", "meta": {"query": "multimodal models in computer vision healthcare case studies"}, "citation_uuid": -1}, "uid_890": {"url": "uid_890", "description": NaN, "snippets": ["an extensive qualitative inspection on various challenging scenes. They\ndemonstrate that our models can produce fairly good scene parsing results for\nrobotics applications. Our code, data and models will be made public."], "title": "", "meta": {"query": "outcomes multimodal models robotics healthcare"}, "citation_uuid": -1}, "uid_4980": {"url": "uid_4980", "description": NaN, "snippets": ["Transformer networks have proven extremely powerful for a wide variety of\ntasks since they were introduced. Computer vision is not an exception, as the\nuse of transformers has become very popular in the vision community in recent\nyears. Despite this wave, multiple-object tracking (MOT) exhibits for now some\nsort of incompatibility with transformers. We argue that the standard\nrepresentation - bounding boxes with insufficient sparse queries - is not"], "title": "", "meta": {"query": "multimodal transformers success in computer vision"}, "citation_uuid": -1}, "uid_1574": {"url": "uid_1574", "description": NaN, "snippets": ["computer vision. In particular, we introduce a Multi-modal Multi-scale\nTRansformer (M2TR), which uses a multi-scale transformer that operates on\npatches of different sizes to detect the local inconsistency at different\nspatial levels. To improve the detection results and enhance the robustness of\nour method to image compression, M2TR also takes frequency information, which\nis further combined with RGB features using a cross modality fusion module."], "title": "", "meta": {"query": "key theoretical advancements in multimodal transformers computer vision"}, "citation_uuid": -1}, "uid_11109": {"url": "uid_11109", "description": NaN, "snippets": ["an extensive qualitative inspection on various challenging scenes. They\ndemonstrate that our models can produce fairly good scene parsing results for\nrobotics applications. Our code, data and models will be made public."], "title": "", "meta": {"query": "outcomes multimodal models robotics healthcare"}, "citation_uuid": -1}, "uid_17244": {"url": "uid_17244", "description": NaN, "snippets": ["the audio-visual video parsing task. Substantial analyses verify that our\nmethods promote the fusion of different modal features."], "title": "", "meta": {"query": "integration of different modalities in computer vision"}, "citation_uuid": -1}, "uid_2006": {"url": "uid_2006", "description": NaN, "snippets": ["methods on the SUN RGB-D dataset. On ScanNet, ImVoxelNet sets a new benchmark\nfor multi-view 3D object detection. The source code and the trained models are\navailable at https://github.com/saic-vul/imvoxelnet."], "title": "", "meta": {"query": "latest advancements in multimodal models for computer vision"}, "citation_uuid": -1}, "uid_1043": {"url": "uid_1043", "description": NaN, "snippets": ["correlation simultaneously, hence improving the quality of learned visual\nrepresentations. By including multimodal training in a unified framework with\ndifferent types of contrastive losses, our method can learn more powerful and\ngeneric visual features. We first train our model on COCO and evaluate the\nlearned visual representations on various downstream tasks including image\nclassification, object detection, and instance segmentation. For example, the"], "title": "", "meta": {"query": "comparison of multimodal models in computer vision"}, "citation_uuid": -1}, "uid_8356": {"url": "uid_8356", "description": NaN, "snippets": ["machine translation, here we extend it to a Multimodal Transformer (MT) model\nfor image captioning. Compared to existing image captioning approaches, the MT\nmodel simultaneously captures intra- and inter-modal interactions in a unified\nattention block. Due to the in-depth modular composition of such attention\nblocks, the MT model can perform complex multimodal reasoning and output\naccurate captions. Moreover, to further improve the image captioning"], "title": "", "meta": {"query": "multimodal models ethical implications 2023"}, "citation_uuid": -1}, "uid_1016": {"url": "uid_1016", "description": NaN, "snippets": ["Recently a number of studies demonstrated impressive performance on diverse\nvision-language multi-modal tasks such as image captioning and visual question\nanswering by extending the BERT architecture with multi-modal pre-training\nobjectives. In this work we explore a broad set of multi-modal representation\nlearning tasks in the medical domain, specifically using radiology images and\nthe unstructured report. We propose Medical Vision Language Learner (MedViLL)"], "title": "", "meta": {"query": "multimodal models in computer vision healthcare case studies"}, "citation_uuid": -1}, "uid_3426": {"url": "uid_3426", "description": NaN, "snippets": ["work evaluates the performance obtained when training convolutional neural\nnetwork models on commonly used driver drowsiness detection datasets and\ntesting on datasets specifically chosen for broader representation. Results\nshow that models trained using publicly available datasets suffer extensively\nfrom over-fitting, and can exhibit racial bias, as shown by testing on a more\nrepresentative dataset. We propose a novel visualisation technique that can"], "title": "", "meta": {"query": "research papers bias multimodal computer vision models"}, "citation_uuid": -1}, "uid_7698": {"url": "uid_7698", "description": NaN, "snippets": ["quantify volumes and shapes used in diagnosis and monitoring treatment;\nregistration of multimodality images of organs improves detection, diagnosis\nand staging of diseases as well as image-guided surgery and therapy,\nregistration of images obtained from the same modality are used to monitor\nprogression of therapy. These challenging clinical-motivated applications\nintroduce novel and sophisticated mathematical problems which stimulate"], "title": "", "meta": {"query": "examples of multimodal models in healthcare applications"}, "citation_uuid": -1}, "uid_15934": {"url": "uid_15934", "description": NaN, "snippets": ["environment and apply it to the real-world. Moreover, our approach, with merely\na 3D model being required, has the potential to generalize to other types of\nmulti-rigid-body dynamic systems."], "title": "", "meta": {"query": "realworld applications of multimodal models in robotics"}, "citation_uuid": -1}, "uid_12957": {"url": "uid_12957", "description": NaN, "snippets": ["environment and apply it to the real-world. Moreover, our approach, with merely\na 3D model being required, has the potential to generalize to other types of\nmulti-rigid-body dynamic systems."], "title": "", "meta": {"query": "realworld applications of multimodal models in robotics"}, "citation_uuid": -1}, "uid_428": {"url": "uid_428", "description": NaN, "snippets": ["quantify volumes and shapes used in diagnosis and monitoring treatment;\nregistration of multimodality images of organs improves detection, diagnosis\nand staging of diseases as well as image-guided surgery and therapy,\nregistration of images obtained from the same modality are used to monitor\nprogression of therapy. These challenging clinical-motivated applications\nintroduce novel and sophisticated mathematical problems which stimulate"], "title": "", "meta": {"query": "examples of multimodal models in healthcare applications"}, "citation_uuid": -1}, "uid_1883": {"url": "uid_1883", "description": NaN, "snippets": ["The inability to interpret the model prediction in semantically and visually\nmeaningful ways is a well-known shortcoming of most existing computer-aided\ndiagnosis methods. In this paper, we propose MDNet to establish a direct\nmultimodal mapping between medical images and diagnostic reports that can read\nimages, generate diagnostic reports, retrieve images by symptom descriptions,\nand visualize attention, to provide justifications of the network diagnosis"], "title": "", "meta": {"query": "multimodal models in healthcare diagnostics technologies"}, "citation_uuid": -1}, "uid_11511": {"url": "uid_11511", "description": NaN, "snippets": ["Real-world scenarios pose several challenges to deep learning based computer\nvision techniques despite their tremendous success in research. Deeper models\nprovide better performance, but are challenging to deploy and knowledge\ndistillation allows us to train smaller models with minimal loss in\nperformance. The model also has to deal with open set samples from classes\noutside the ones it was trained on and should be able to identify them as"], "title": "", "meta": {"query": "impact of challenges on multimodal model effectiveness in realworld scenarios"}, "citation_uuid": -1}, "uid_11628": {"url": "uid_11628", "description": NaN, "snippets": ["Robust multiple model fitting plays a crucial role in many computer vision\napplications. Unlike single model fitting problems, the multi-model fitting has\nadditional challenges. The unknown number of models and the inlier noise scale\nare the two most important of them, which are in general provided by the user\nusing ground-truth or some other auxiliary information. Mode seeking/\nclustering-based approaches crucially depend on the quality of model hypotheses"], "title": "", "meta": {"query": "ethical concerns multimodal models computer vision"}, "citation_uuid": -1}, "uid_17024": {"url": "uid_17024", "description": NaN, "snippets": ["Robust multiple model fitting plays a crucial role in many computer vision\napplications. Unlike single model fitting problems, the multi-model fitting has\nadditional challenges. The unknown number of models and the inlier noise scale\nare the two most important of them, which are in general provided by the user\nusing ground-truth or some other auxiliary information. Mode seeking/\nclustering-based approaches crucially depend on the quality of model hypotheses"], "title": "", "meta": {"query": "ethical concerns multimodal models computer vision"}, "citation_uuid": -1}, "uid_16467": {"url": "uid_16467", "description": NaN, "snippets": ["For many computer vision applications, such as image description and human\nidentification, recognizing the visual attributes of humans is an essential yet\nchallenging problem. Its challenges originate from its multi-label nature, the\nlarge underlying class imbalance and the lack of spatial annotations. Existing\nmethods follow either a computer vision approach while failing to account for\nclass imbalance, or explore machine learning solutions, which disregard the"], "title": "", "meta": {"query": "case studies multimodal models computer vision fairness issues"}, "citation_uuid": -1}, "uid_9266": {"url": "uid_9266", "description": NaN, "snippets": ["Image captioning is an important task for benchmarking visual reasoning and\nfor enabling accessibility for people with vision impairments. However, as in\nmany machine learning settings, social biases can influence image captioning in\nundesirable ways. In this work, we study bias propagation pathways within image\ncaptioning, focusing specifically on the COCO dataset. Prior work has analyzed\ngender bias in captions using automatically-derived gender labels; here we"], "title": "", "meta": {"query": "societal implications of biases in multimodal models computer vision"}, "citation_uuid": -1}, "uid_16902": {"url": "uid_16902", "description": NaN, "snippets": ["Algorithmic decision systems have frequently been labelled as \"biased\",\n\"racist\", \"sexist\", or \"unfair\" by numerous media outlets, organisations, and\nresearchers. There is an ongoing debate about whether such assessments are\njustified and whether citizens and policymakers should be concerned. These and\nother related matters have recently become a hot topic in the context of\nbiometric technologies, which are ubiquitous in personal, commercial, and"], "title": "", "meta": {"query": "current issues in multimodal AI models ethics"}, "citation_uuid": -1}, "uid_14040": {"url": "uid_14040", "description": NaN, "snippets": ["Modern machine learning models for computer vision exceed humans in accuracy\non specific visual recognition tasks, notably on datasets like ImageNet.\nHowever, high accuracy can be achieved in many ways. The particular decision\nfunction found by a machine learning system is determined not only by the data\nto which the system is exposed, but also the inductive biases of the model,\nwhich are typically harder to characterize. In this work, we follow a recent"], "title": "", "meta": {"query": "biases in computer vision models law enforcement implications"}, "citation_uuid": -1}, "uid_5183": {"url": "uid_5183", "description": NaN, "snippets": ["Modern machine learning models for computer vision exceed humans in accuracy\non specific visual recognition tasks, notably on datasets like ImageNet.\nHowever, high accuracy can be achieved in many ways. The particular decision\nfunction found by a machine learning system is determined not only by the data\nto which the system is exposed, but also the inductive biases of the model,\nwhich are typically harder to characterize. In this work, we follow a recent"], "title": "", "meta": {"query": "examples of bias in multimodal models in computer vision"}, "citation_uuid": -1}}}