{"url_to_unified_index": {"uid_15937": 3, "uid_1351": 8, "uid_4980": 6, "uid_4835": 7, "uid_4834": 2, "uid_2316": 1, "uid_1016": 4, "uid_1641": 5, "uid_2877": 9, "uid_13885": 16, "uid_14460": 17, "uid_16902": 18, "uid_5516": 15, "uid_15934": 19, "uid_12957": 20, "uid_16322": 21, "uid_9975": 22, "uid_5333": 10, "uid_11146": 23, "uid_1883": 11, "uid_1043": 12, "uid_8356": 13, "uid_16002": 14}, "url_to_info": {"uid_15937": {"url": "uid_15937", "description": NaN, "snippets": ["combining specifications and visual information of items have succeeded in\r\nobtaining fine-grained representations of fashion items, although they\r\ngenerally apply simple vector operations through a multimodal fusion. We\r\nsimilarly build a multimodal model using images and attributes of the product\r\nand further employ state-of-the-art multimodal deep neural networks applied in\r\ncomputer vision to achieve a practical performance level. In addition, we model", "combining specifications and visual information of items have succeeded in\nobtaining fine-grained representations of fashion items, although they\ngenerally apply simple vector operations through a multimodal fusion. We\nsimilarly build a multimodal model using images and attributes of the product\nand further employ state-of-the-art multimodal deep neural networks applied in\ncomputer vision to achieve a practical performance level. In addition, we model"], "title": "", "meta": {"query": "multimodal models in computer vision applications examples"}, "citation_uuid": -1}, "uid_1351": {"url": "uid_1351", "description": NaN, "snippets": ["Multimodal representation learning is gaining more and more interest within\r\nthe deep learning community. While bilinear models provide an interesting\r\nframework to find subtle combination of modalities, their number of parameters\r\ngrows quadratically with the input dimensions, making their practical\r\nimplementation within classical deep learning pipelines challenging. In this\r\npaper, we introduce BLOCK, a new multimodal fusion based on the"], "title": "", "meta": {"query": "impact of multimodal models on industries"}, "citation_uuid": -1}, "uid_4980": {"url": "uid_4980", "description": NaN, "snippets": ["Transformer networks have proven extremely powerful for a wide variety of\ntasks since they were introduced. Computer vision is not an exception, as the\nuse of transformers has become very popular in the vision community in recent\nyears. Despite this wave, multiple-object tracking (MOT) exhibits for now some\nsort of incompatibility with transformers. We argue that the standard\nrepresentation - bounding boxes with insufficient sparse queries - is not", "Transformer networks have proven extremely powerful for a wide variety of\r\ntasks since they were introduced. Computer vision is not an exception, as the\r\nuse of transformers has become very popular in the vision community in recent\r\nyears. Despite this wave, multiple-object tracking (MOT) exhibits for now some\r\nsort of incompatibility with transformers. We argue that the standard\r\nrepresentation - bounding boxes with insufficient sparse queries - is not"], "title": "", "meta": {"query": "multimodal transformers success in computer vision"}, "citation_uuid": -1}, "uid_4835": {"url": "uid_4835", "description": NaN, "snippets": ["The recent success of Transformers in the language domain has motivated\nadapting it to a multimodal setting, where a new visual model is trained in\ntandem with an already pretrained language model. However, due to the excessive\nmemory requirements from Transformers, existing work typically fixes the\nlanguage model and train only the vision module, which limits its ability to\nlearn cross-modal information in an end-to-end manner. In this work, we focus"], "title": "", "meta": {"query": "limitations of multimodal models in robotics"}, "citation_uuid": -1}, "uid_4834": {"url": "uid_4834", "description": NaN, "snippets": ["largely improved performance in simultaneous 2D object localization and\r\nviewpoint estimation on a recent dataset of challenging street scenes.", "largely improved performance in simultaneous 2D object localization and\nviewpoint estimation on a recent dataset of challenging street scenes."], "title": "", "meta": {"query": "recent advancements in multimodal models for computer vision tasks"}, "citation_uuid": -1}, "uid_2316": {"url": "uid_2316", "description": NaN, "snippets": ["correlation simultaneously, hence improving the quality of learned visual\nrepresentations. By including multimodal training in a unified framework with\ndifferent types of contrastive losses, our method can learn more powerful and\ngeneric visual features. We first train our model on COCO and evaluate the\nlearned visual representations on various downstream tasks including image\nclassification, object detection, and instance segmentation. For example, the"], "title": "", "meta": {"query": "comparison of multimodal models in computer vision"}, "citation_uuid": -1}, "uid_1016": {"url": "uid_1016", "description": NaN, "snippets": ["Recently a number of studies demonstrated impressive performance on diverse\nvision-language multi-modal tasks such as image captioning and visual question\nanswering by extending the BERT architecture with multi-modal pre-training\nobjectives. In this work we explore a broad set of multi-modal representation\nlearning tasks in the medical domain, specifically using radiology images and\nthe unstructured report. We propose Medical Vision Language Learner (MedViLL)"], "title": "", "meta": {"query": "multimodal models in computer vision healthcare case studies"}, "citation_uuid": -1}, "uid_1641": {"url": "uid_1641", "description": NaN, "snippets": ["The focus of this survey is on the analysis of two modalities of multimodal\r\ndeep learning: image and text. Unlike classic reviews of deep learning where\r\nmonomodal image classifiers such as VGG, ResNet and Inception module are\r\ncentral topics, this paper will examine recent multimodal deep models and\r\nstructures, including auto-encoders, generative adversarial nets and their\r\nvariants. These models go beyond the simple image classifiers in which they can", "The focus of this survey is on the analysis of two modalities of multimodal\ndeep learning: image and text. Unlike classic reviews of deep learning where\nmonomodal image classifiers such as VGG, ResNet and Inception module are\ncentral topics, this paper will examine recent multimodal deep models and\nstructures, including auto-encoders, generative adversarial nets and their\nvariants. These models go beyond the simple image classifiers in which they can"], "title": "", "meta": {"query": "theoretical foundations of multimodal models in vision"}, "citation_uuid": -1}, "uid_2877": {"url": "uid_2877", "description": NaN, "snippets": ["distorted. These rare \"edge-case\" scenarios are not represented in available\r\ndatasets, and existing fusion architectures are not designed to handle them. To\r\naddress this challenge we present a novel multimodal dataset acquired in over\r\n10,000km of driving in northern Europe. Although this dataset is the first\r\nlarge multimodal dataset in adverse weather, with 100k labels for lidar,\r\ncamera, radar, and gated NIR sensors, it does not facilitate training as", "distorted. These rare \"edge-case\" scenarios are not represented in available\ndatasets, and existing fusion architectures are not designed to handle them. To\naddress this challenge we present a novel multimodal dataset acquired in over\n10,000km of driving in northern Europe. Although this dataset is the first\nlarge multimodal dataset in adverse weather, with 100k labels for lidar,\ncamera, radar, and gated NIR sensors, it does not facilitate training as"], "title": "", "meta": {"query": "notable incidents involving multimodal models in surveillance and law enforcement"}, "citation_uuid": -1}, "uid_13885": {"url": "uid_13885", "description": NaN, "snippets": ["the impact of different datasets, models, attack methods and defense methods.\nDirectly, our work proves the limited robustness of unsupervised domain\nadaptation model, and we hope our work may facilitate the community to pay more\nattention to improve the robustness of the model against attacking."], "title": "", "meta": {"query": "societal impact of multimodal AI models in law enforcement"}, "citation_uuid": -1}, "uid_14460": {"url": "uid_14460", "description": NaN, "snippets": ["methods only focus on improving accuracy with minimal parameter update, while\nignoring high computing and memory cost during training, which makes it\nimpossible to deploy multi-domain learning into more and more widely used\nresource-limited edge devices, like mobile phone, IoT, embedded system, etc.\nDuring our study in multi-domain training, we observe that large memory used\nfor activation storage is the bottleneck that largely limits the training time"], "title": "", "meta": {"query": "limitations of multimodal models in industry applications"}, "citation_uuid": -1}, "uid_16902": {"url": "uid_16902", "description": NaN, "snippets": ["Algorithmic decision systems have frequently been labelled as \"biased\",\n\"racist\", \"sexist\", or \"unfair\" by numerous media outlets, organisations, and\nresearchers. There is an ongoing debate about whether such assessments are\njustified and whether citizens and policymakers should be concerned. These and\nother related matters have recently become a hot topic in the context of\nbiometric technologies, which are ubiquitous in personal, commercial, and"], "title": "", "meta": {"query": "current issues in multimodal AI models ethics"}, "citation_uuid": -1}, "uid_5516": {"url": "uid_5516", "description": NaN, "snippets": ["The observation that computer vision methods overfit to dataset specifics has\r\ninspired diverse attempts to make object recognition models robust to domain\r\nshifts. However, similar work on domain-robust visual question answering\r\nmethods is very limited. Domain adaptation for VQA differs from adaptation for\r\nobject recognition due to additional complexity: VQA models handle multimodal\r\ninputs, methods contain multiple steps with diverse modules resulting in", "The observation that computer vision methods overfit to dataset specifics has\ninspired diverse attempts to make object recognition models robust to domain\nshifts. However, similar work on domain-robust visual question answering\nmethods is very limited. Domain adaptation for VQA differs from adaptation for\nobject recognition due to additional complexity: VQA models handle multimodal\ninputs, methods contain multiple steps with diverse modules resulting in"], "title": "", "meta": {"query": "limitations of multimodal models in computer vision"}, "citation_uuid": -1}, "uid_15934": {"url": "uid_15934", "description": NaN, "snippets": ["environment and apply it to the real-world. Moreover, our approach, with merely\r\na 3D model being required, has the potential to generalize to other types of\r\nmulti-rigid-body dynamic systems.", "environment and apply it to the real-world. Moreover, our approach, with merely\na 3D model being required, has the potential to generalize to other types of\nmulti-rigid-body dynamic systems."], "title": "", "meta": {"query": "realworld applications of multimodal models in robotics"}, "citation_uuid": -1}, "uid_12957": {"url": "uid_12957", "description": NaN, "snippets": ["environment and apply it to the real-world. Moreover, our approach, with merely\r\na 3D model being required, has the potential to generalize to other types of\r\nmulti-rigid-body dynamic systems.", "environment and apply it to the real-world. Moreover, our approach, with merely\na 3D model being required, has the potential to generalize to other types of\nmulti-rigid-body dynamic systems."], "title": "", "meta": {"query": "realworld applications of multimodal models in robotics"}, "citation_uuid": -1}, "uid_16322": {"url": "uid_16322", "description": NaN, "snippets": ["into the behavior of such models. Our results also show examples of unusual\r\nbehaviors learned by models in attempting VQA tasks."], "title": "", "meta": {"query": "case studies algorithmic bias multimodal models"}, "citation_uuid": -1}, "uid_9975": {"url": "uid_9975", "description": NaN, "snippets": ["into the behavior of such models. Our results also show examples of unusual\r\nbehaviors learned by models in attempting VQA tasks."], "title": "", "meta": {"query": "case studies algorithmic bias multimodal models"}, "citation_uuid": -1}, "uid_5333": {"url": "uid_5333", "description": NaN, "snippets": ["Recently a number of studies demonstrated impressive performance on diverse\nvision-language multi-modal tasks such as image captioning and visual question\nanswering by extending the BERT architecture with multi-modal pre-training\nobjectives. In this work we explore a broad set of multi-modal representation\nlearning tasks in the medical domain, specifically using radiology images and\nthe unstructured report. We propose Medical Vision Language Learner (MedViLL)"], "title": "", "meta": {"query": "multimodal models in computer vision healthcare case studies"}, "citation_uuid": -1}, "uid_11146": {"url": "uid_11146", "description": NaN, "snippets": ["current handcrafted deep models on challenging benchmarks across multiple tasks\r\nwith remarkable margins."], "title": "", "meta": {"query": "current issues in multimodal AI models ethics"}, "citation_uuid": -1}, "uid_1883": {"url": "uid_1883", "description": NaN, "snippets": ["The inability to interpret the model prediction in semantically and visually\nmeaningful ways is a well-known shortcoming of most existing computer-aided\ndiagnosis methods. In this paper, we propose MDNet to establish a direct\nmultimodal mapping between medical images and diagnostic reports that can read\nimages, generate diagnostic reports, retrieve images by symptom descriptions,\nand visualize attention, to provide justifications of the network diagnosis"], "title": "", "meta": {"query": "multimodal models in healthcare diagnostics technologies"}, "citation_uuid": -1}, "uid_1043": {"url": "uid_1043", "description": NaN, "snippets": ["correlation simultaneously, hence improving the quality of learned visual\r\nrepresentations. By including multimodal training in a unified framework with\r\ndifferent types of contrastive losses, our method can learn more powerful and\r\ngeneric visual features. We first train our model on COCO and evaluate the\r\nlearned visual representations on various downstream tasks including image\r\nclassification, object detection, and instance segmentation. For example, the"], "title": "", "meta": {"query": "comparison of multimodal models in computer vision"}, "citation_uuid": -1}, "uid_8356": {"url": "uid_8356", "description": NaN, "snippets": ["machine translation, here we extend it to a Multimodal Transformer (MT) model\r\nfor image captioning. Compared to existing image captioning approaches, the MT\r\nmodel simultaneously captures intra- and inter-modal interactions in a unified\r\nattention block. Due to the in-depth modular composition of such attention\r\nblocks, the MT model can perform complex multimodal reasoning and output\r\naccurate captions. Moreover, to further improve the image captioning"], "title": "", "meta": {"query": "multimodal models ethical implications 2023"}, "citation_uuid": -1}, "uid_16002": {"url": "uid_16002", "description": NaN, "snippets": ["machine translation, here we extend it to a Multimodal Transformer (MT) model\r\nfor image captioning. Compared to existing image captioning approaches, the MT\r\nmodel simultaneously captures intra- and inter-modal interactions in a unified\r\nattention block. Due to the in-depth modular composition of such attention\r\nblocks, the MT model can perform complex multimodal reasoning and output\r\naccurate captions. Moreover, to further improve the image captioning"], "title": "", "meta": {"query": "multimodal models ethical implications 2023"}, "citation_uuid": -1}}}